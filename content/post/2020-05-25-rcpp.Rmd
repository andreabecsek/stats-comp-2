---
title: Rcpp
author: Andrea Becsek
date: '2020-05-25'
slug: rcpp
categories: []
tags:
  - rcpp
---

The aim of this portfolio is to demonstrate the usage of `C++` in conjunction with `R` via the `Rcpp` package. The application that we are considering is the adaptive kernel regression smoothing.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(Rcpp)
library(microbenchmark)
library(ggplot2)
library(dplyr)
theme_set(theme_minimal())
```

Simulate data generated by the model
$$y_i = sin(\alpha\pi x^3)+z_i, \;\; \text{where}\;z_i \sim\mathcal{N}(0,\sigma^2),$$

for $i=1, \cdots, 400$, $\alpha=4$, and $\sigma=0.2$.



```{r}
set.seed(42)
n <- 400
x <- runif(n)
y <- sin(4 * pi * x^3) + rnorm(n, 0, 0.2)
df <- data.frame(x = x, y = y)
```

Estimate the conditional expectation $\mu(x)=\mathbb{E}(y|x)$ using a kernel regression smoother
$$\hat{\mu}(x)=\frac{\sum_{i=1}^nk_{\lambda}(x,x_i)y_i}{\sum_{i=1}^nk_{\lambda}(x,x_i)},$$
where $\lambda>0$. The estimation is produced by using a Gaussian kernel with variance $\lambda^2$.

Let's first implement the function in `R`:
```{r}
meanKRS <- function(y, x, x0, lam) {
  n <- length(x)
  n0 <- length(x0)
  out <- numeric(n0)

  # compute the estimated conditional expectation
  for (i in 1:n0) {
    kernel <- dnorm(x, x0[i], lam)
    out[i] <- sum(kernel * y) / sum(kernel)
  }

  return(out)
}
```

Use the estimator to produce two fits, for two different $\lambda$ values
```{r}
# test points
x_new <- seq(0, 1, length.out = 1000)

# fit estimator for large lambda
muSmoothLarge <- meanKRS(y = y, x = x, x0 = x_new, lam = 0.06)

# fit estimator for small lambda
muSmoothSmall <- meanKRS(y = y, x = x, x0 = x_new, lam = 0.02)
```

Plot data and the two fits for the different lambdas.
```{r echo=FALSE} 
df <- data.frame(df)
df_new <- data.frame(x_new, muSmoothLarge, muSmoothSmall)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLarge, colour = "large"), size = 1) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmall, colour = "small"), size = 1) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1")) +
  labs(title = "Fitted smooths using R implementation")
```

### Q1a

A `C` implementation of the `meanKRS` function.

```{Rcpp eval = FALSE}
// include required headers
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export(name = "meanKRSRcpp")]]
NumericVector meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  
  // define required variables and output vector
	int n = x.size();
	int n0 = x0.size();
	NumericVector out(n0);
	
	// estimate conditional expectation
	for(int i = 1; i < n0; i++){
	  NumericVector kernel = dnorm(x, x0[i],lambda,0);
	  
	  // numerator
	  double num = 0;
	  for(int j = 1; j < n; j++){
	    num = num + kernel[j] * y[j];
	  }
	  
	  //denominator
	  double denom = 0;
	  for(int j = 1; j < n; j++){
	    denom = denom + kernel[j];
	  }
	  
  out[i] = num / denom;
	}
	
	return out;
}
```

Source the file containing the function above.

```{r}
sourceCpp("../code/meanKRS.cpp")
```

Fit the data using the `C` function.

```{r}
muSmoothLargeRcpp <- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)
muSmoothSmallRcpp <- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.02)
```

Plot the two fits

```{r echo = FALSE}
df_new <- data.frame(x_new, muSmoothLargeRcpp, muSmoothSmallRcpp)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLargeRcpp, colour = "large"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmallRcpp, colour = "small"), size = 0.8) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1")) +
  labs(title = "Fitted smooths using C implementation")
```

Check whether the two function give the same results. If the error is roughly $10^{-15}$, then the error is mainly due to machine precision.

```{r}
max(abs(meanKRS(y = y, x = x, x0 = x_new, lam = 0.06) - meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)))
```

Compare the efficiency of the two functions and note that the `Rcpp` implementation is about $1.5$ times faster.

```{r message=FALSE}
microbenchmark(meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), meanKRS(y = y, x = x, x0 = x_new, lam = 0.06), unit = "relative")
```

### Q1b

Since choosing $\lambda$ requires a more formal way to do so, we implement a cross validation function to use.

```{r}
cv_error <- function(y, x, lambdas, k = 10) {
  # define error metric
  mse <- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors <- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds <- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error <- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test <- x[which(folds == fold)]
      y_test <- y[which(folds == fold)]
      x_train <- x[which(folds != fold)]
      y_train <- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit <- meanKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error <- error + (mse(fit, y_test) / k)
    }
    errors[i] <- error
  }
  return(errors)
}
```

Find the lambda which has the smallest cross-validation error.
```{r}
lambdas <- seq(0.001, 0.025, length.out = 50)
errors_krs <- cv_error(y, x, lambdas, 10)
# lambda_min_krs <- lambdas[which.min(cv)]
```


Plot the cross validation error curve and the lambda that gives the smallest error.
```{r}
data.frame(lambda = lambdas, error = errors_krs) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21,size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

`C` implementation of cross validation.

```{rcpp eval = FALSE}
// mean squared error
double mse(const NumericVector a, const NumericVector b){
  return mean(pow((a-b),2));
}

// random wrapper
int randWrapper(const int n){
  return(floor(unif_rand()));
}

// create a sequence of n samples from 0 to k
// [[Rcpp::export(name = "sample_rcpp")]]
NumericVector sample_rcpp(int k, int n){
  NumericVector seq(n);

  // create ordered sequence
  for(int i = 1; i <= k; i++){
    for(int j = i; j <= n; j= j + k){
      seq[j-1] = i-1;
    }
  }

  // randomly shuffle the sequence of indeces
  std::random_shuffle(seq.begin(), seq.end(), randWrapper);
  return seq;
}

// cross validation function

// [[Rcpp::export(name = "cv_meanKRS_cpp")]]
NumericVector cv_meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector lambdas, const int k){
  int n = x.size();
  int m = lambdas.size();

  NumericVector errors(m);
  NumericVector folds = sample_rcpp(k, n);

  // iterate over lambdas
  for(int i=0; i < m; i++){
    double error = 0;

    // iterate over folds
    for(int j = 0; j < k; j++){

      // split the data into train and test set
      NumericVector x_test = x[(folds==j)];
      NumericVector x_train = x[(folds!=j)];
      NumericVector y_test = y[(folds==j)];
      NumericVector y_train = y[(folds!=j)];

      // fit model on training data and get
      NumericVector fit = meanKRSR_cpp(y_train, x_train, x_test, lambdas[i]);

      // calculate error for fold
      error = error + mse(fit, y_test)/k;
    }
    errors[i] = error;
  }

  return errors;
}
```

Source the `C` file.

```{r}
sourceCpp("../code/cv_meanKRS.cpp")
```

Use the `C` implementation to perform the cross validation.
```{r}
errors_krs_cpp <- cv_meanKRS_cpp(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r}
data.frame(lambda = lambdas, error = errors_krs_cpp) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

Compare the efficiency of the `R` and `C` implementations.

```{r eval=FALSE}
microbenchmark(cv_meanKRS_cpp(y, x, lambdas, 5), cv_error(y, x, lambdas, 5), unit = "relative")
```

###Q2

Implement the KRS with a variable $\lambda$ value.

```{r}
varKRS <- function(y, x, x0, lam) {
  n <- length(x)
  n0 <- length(x0)
  mu <- numeric(n)

  out <- numeric(n0)
  madHat <- numeric(n0)

  for (i in 1:n) {
    kernel <- dnorm(x, x[i], lam)
    mu[i] <- sum(kernel * y) / sum(kernel)
  }

  resAbs <- abs(y - mu)
  for (ii in 1:n0) {
    kernel <- dnorm(x, x0[ii], lam)
    madHat[ii] <- sum(kernel * resAbs) / sum(kernel)
  }

  w <- 1 / madHat
  w <- w / mean(w)

  for (i in 1:n0) {
    kernel <- dnorm(x, x0[i], lam * w[i])
    out[i] <- sum(kernel * y) /
      sum(kernel)
  }

  return(out)
}
```

Fit the adaptive smooth using the `R` implementation.

```{r}
muSmoothAdapt <- varKRS(y = y, x = x, x0 = x_new, lam = 0.06)
```

Plot the fit of all smooths.

```{r}
df_new <- data.frame(x_new, muSmoothLarge, muSmoothSmall, muSmoothAdapt)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLarge, colour = "large"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmall, colour = "small"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothAdapt, colour = "adapt"), size = 0.8) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1", adapt = "#ef3939")) +
  labs(title = "Fitted smooths using R implementation")
```

Writing the function in `C`
```{rcpp eval=FALSE}
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export(name = "varKRS_cpp")]]
NumericVector varKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  int n = x.size();
  int n0 = x0.size();
  NumericVector res(n);
  NumericVector mu(n);

  NumericVector madHat(n0);
  NumericVector out(n0);

  for(int i = 0; i < n; i++){
    NumericVector kernel = dnorm(x, x[i],lambda,0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    mu[i] = num / denom;
  }

  NumericVector resAbs(n);
  resAbs = abs(y - mu);


  NumericVector w(n0);
  double sum_w = 0;

  for(int i=0; i < n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda,0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * resAbs[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    madHat[i] = num / denom;
    w[i] = denom / num;
    sum_w = sum_w + w[i];

  }

  for(int i = 0; i < n0; i++){
    w[i] = n0 * w[i] / sum_w;
  }

  for(int i=0; i < n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda*w[i],0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    out[i] = num / denom;
  }


  return out;
}
```

Source the `C` file for the above function.

```{r}
sourceCpp("../code/varKRS.cpp")
```

Fit the model.
```{r}
muSmoothAdapt_rcpp <- varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)
```

Check that it returns the same fitted values as the `R` implementation.
```{r}
max(abs(muSmoothAdapt - muSmoothAdapt_rcpp))
```

Comparing the performances it is clear that the `C` implementation is faster.
```{r}
microbenchmark(varKRS(y = y, x = x, x0 = x_new, lam = 0.06), varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), unit = "relative")
```

Perform cross validation using `R`
```{r}
cv_error_var <- function(y, x, lambdas, k = 10) {
  # define error metric
  mse <- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors <- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds <- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error <- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test <- x[which(folds == fold)]
      y_test <- y[which(folds == fold)]
      x_train <- x[which(folds != fold)]
      y_train <- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit <- varKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error <- error + (mse(fit, y_test) / k)
    }
    errors[i] <- error
  }
  return(errors)
}
```

Compute the errors
```{r}
errors_var <- cv_error_var(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r warning=FALSE, message=FALSE}
data.frame(lambda = lambdas, error = errors_var) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5)
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

Comparing the performance of the `R` and the `C` implementation of the cross validation we get

```{r}
sourceCpp("../code/cv_varKRS.cpp")
```

Compute the errors.
```{r}
errors_var_cpp <- cv_varKRS_cpp(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r warning=FALSE, message=FALSE}
data.frame(lambda = lambdas, error = errors_var_cpp) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey", linetype='dashed')
```

```{r eval=FALSE}
microbenchmark(cv_varKRS_cpp(y, x, lambdas, 5), cv_error_var(y, x, lambdas, 5), unit = "relative")
```


## Using Rcpp in a package

Now that we have seen how to write code in Rcpp, we might want to make this more reproducible by embedding everything into a package.

Luckily, `Rcpp` provides an `Rcpp.package.skeleton` function which, as the name says, creates a skeleton package. After creating this skeleton, we can examine what has been generated:
```{r message=FALSE, eval=FALSE}
# PACKAGE FOR ADAPTIVE KERNEL REGRESSION SMOOTHING
Rcpp.package.skeleton('akrs')
system("ls -1R akrs/")
```

Now, let's examine the `rcpp_hello_world.cpp` file
```{Rcpp, eval=FALSE}

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List rcpp_hello_world() {

    CharacterVector x = CharacterVector::create( "foo", "bar" )  ;
    NumericVector y   = NumericVector::create( 0.0, 1.0 ) ;
    List z            = List::create( x, y ) ;

    return z ;
}
```

This file has the structure that we are now used to, but note that we use `// [[Rcpp::export]]` to export the `rcpp_hello_world()` function. In order for this to happen, the `compileAttributes` function is called and it generates the `RcppExports.cpp`, which is the Rcpp wrapper for the function. The R wrapper can be found in `RcppExports.R`.

The **DESCRIPTION** file looks like this so far:
```{r eval=FALSE}
Package: akrs
Type: Package
Title: What the Package Does
   in One 'Title Case' Line
Version: 1.0
Date: 2020-06-16
Author: Your Name
Maintainer: Your Name
   <your@email.com>
Description: One paragraph
   description of what the
   package does as one or
   more full sentences.
License: GPL (>= 2)
Imports: Rcpp (>= 1.0.4)
LinkingTo: Rcpp
```

* `Imports`: indicates dependency between package and `Rcpp`
* `LinkingTo`: package uses header files from specified package (i.e. `Rcpp`)

The **NAMESPACE** file looks as follows:
```{r eval=FALSE}
# ENABLE ALL REGISTERED ROUTINES TO BE ACCESSIBLE FROM R
useDynLib(akrs, .registration=TRUE)

# EXPORTS ALL FUNCTIONS THAT START WITH A LETTER
exportPattern("^[[:alpha:]]+")

# PACKAGES USING RCPP NEED TO IMPORT SOMETHING FROM IT (MIGHT NOT BE REQUIRED IN THE FUTURE)
importFrom(Rcpp, evalCpp)
```

### Add `meanKRS` function to the package.

The `meanKRS` function looks like this
```{Rcpp eval=FALSE}
#include <Rcpp.h>

using namespace Rcpp;

//'Estimate conditional expectiation using a Gaussian Kernel with fixed variance.
//'
//' @param y numeric vector
//' @param x numeric vector
//' @param x0 numeric vector
//' @param lambda double
//' @return conditional expectation
// [[Rcpp::export(name = "meanKRS_cpp")]]
NumericVector meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  // define required variables and output vector
  int n = x.size();
	int n0 = x0.size();
	NumericVector out(n0);
	
	// estimate conditional expectation
	for(int i = 0; i < n0; i++){
	  NumericVector kernel = dnorm(x, x0[i],lambda,0);
	  
	  // numerator
	  double num = 0;
	  for(int j = 0; j < n; j++){
	    num = num + kernel[j] * y[j];
	  }
	  
	  //denominator
	  double denom = 0;
	  for(int j = 0; j < n; j++){
	    denom = denom + kernel[j];
	  }
  out[i] = num / denom;
	}
	return out;
}
```

Note that we have added some comments to be converted into documentation using `roxygenize('akrs')`. This file has been added to the `src` folder.

Let's compile the `Rcpp` attributes:
```{r eval=FALSE}
compileAttributes("akrs")
```

The obtained wrapper function is in `akrs/R/RcppExports.R`:
```{r eval=FALSE}
# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#'Estimate conditional expectation using a Gaussian Kernel with fixed variance.
#'
#' @param y numeric vector
#' @param x numeric vector
#' @param x0 numeric vector
#' @param lambda double
#' @return conditional expectation
meanKRS_cpp <- function(y, x, x0, lambda) {
    .Call(`_akrs_meanKRS_cpp`, y, x, x0, lambda)
}

rcpp_hello_world <- function() {
    .Call(`_akrs_rcpp_hello_world`)
}
```

We could now try to build our package using
```{r eval=FALSE}
system('R CMD build akrs')
```

And once the build is successful, the package can be installed using
```{r eval=FALSE}
system('R CMD INSTALL akrs_1.0.tar.gz')
```

Similarly, we have also added the `varKRS_cpp` to the package. 

### Make the C++ functions from the package callable by other packages.
Let's say the package that we have created is an already existing package created  by someone else, and we want to write our package using its functions. In this case, while this might not be the ideal approach, we demonstrate how to achieve this by creating a cross-validation package already existing `akrs` package.

One might think that it is a good idea to call the `C++` functions from an `R` package via `.Call` is not encouraged. We want to get the functions communicating via `C++`. The `Rcpp::interfaces(cpp)` command is essentially used to export a `C++` function from the package as follows:

```{Rcpp eval=FALSE}
#include <Rcpp.h>

// [[Rcpp::interfaces(R, cpp)]]

using namespace Rcpp;

//'Estimate conditional expectation using a Gaussian Kernel with fixed variance.
//'
//' @param y numeric vector
//' @param x numeric vector
//' @param x0 numeric vector
//' @param lambda double
//' @return conditional expectation
// [[Rcpp::export(name = "meanKRS_cpp")]]
NumericVector meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  // define required variables and output vector
  int n = x.size();
  int n0 = x0.size();
  NumericVector out(n0);
  
  // estimate conditional expectation
  for(int i = 0; i < n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda,0);
    
    // numerator
    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * y[j];
    }
    
    //denominator
    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    out[i] = num / denom;
  }
  return out;
}
```

Recompiling the attribues, the following lines appear in `RcppExports.cpp`:
```{Rcpp eval=FALSE}
// registerCCallable (register entry points for exported C++ functions)
RcppExport SEXP _akrs_RcppExport_registerCCallable() { 
    R_RegisterCCallable("akrs", "_akrs_meanKRS_cpp", (DL_FUNC)_akrs_meanKRS_cpp_try);
    R_RegisterCCallable("akrs", "_akrs_RcppExport_validate", (DL_FUNC)_akrs_RcppExport_validate);
    return R_NilValue;
}
```

This simply shows that the `_akrs_meanKRS_cpp` has been registered and can be called from other packages. 

Moreover, a header files have also been created

* `akrs.h`: header file for the package that includes the associated header files, in this case:
* `akrs_RcppExports.h`: defines the package NAMESPACE and exports the function with the name that we specified for it in using `Rcpp::export(name = "meanKRS_cpp")`.

Now, the function can be called from other packages. To demonstrate this, we create the cross-validation package mentioned above, and call it `akrscv`.

```{r eval=FALSE}
Rcpp.package.skeleton("akrscv")
```

We make sure to link this package to the `akrs` package by 
adding `akrscv` to the `LinkingTo` filed in the DESCRIPTION file.

We add the cross-validation function `cv_meanKRS.cpp` and include the header of the linked package using `#include <akrs.h>`. Rebuild and install the `akrs` package using
```{r eval=FALSE}
system("R CMD build akrs")
system("R CMD INSTALL akrs_1.0.tar.gz")
```

Finally, compile the attributes of `akrscv`, build and compile.
```{r eval=FALSE}
compileAttributes('akrscv')
system("R CMD build akrscv")
system("R CMD INSTALL akrscv_1.0.tar.gz")
```

Note: If the man file for a function does not appear, do not write it by hand. Rebuild and use `roxygen2::roxygenize()`.

Let's check if the function does indeed work.

1. Install package from github using
```{r message=FALSE, wraning=FALSE}
devtools::install_github("andreabecsek/akrs", force=TRUE)
devtools::install_github("andreabecsek/akrscv", force=TRUE)
```

Use the package functions to check whether they give the right results
```{r warning=FALSE}
R_result <- meanKRS(y = y, x = x, x0 = x_new, lam = 0.06)
package_result <- akrs::meanKRS_cpp(y,x,x_new,0.06)
mean(abs(R_result-package_result))
```

Check `akrscv` package. Note that the results given by the two methods are no longer similar to machine precision, this is because cross-validation multiple probabilitic choices, so we cannot expect the same results.
```{r message=FALSE, warning=FALSE}
lambdas <- seq(0.001, 0.025, length.out = 50)
R_cv_result <- cv_error(y, x, lambdas, 10)
package_cv_result <- akrscv::cv_meanKRS_cpp(y,x,lambdas, 10)
mean(abs(R_cv_result-package_cv_result))
```

Both packages seem to be working fine and to be giving the correct results.
