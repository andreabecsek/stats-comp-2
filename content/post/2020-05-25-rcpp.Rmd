---
title: Rcpp
author: Andrea Becsek
date: '2020-05-25'
slug: rcpp
categories: []
tags:
  - rcpp
---

The aim of this portfolio is to demonstrate the usage of `C++` in conjunction with `R` via the `Rcpp` package. The application that we are considering is the adaptive kernel regression smoothing.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(Rcpp)
library(microbenchmark)
library(ggplot2)
library(dplyr)
theme_set(theme_minimal())
```

Simulate data generated by the model
$$y_i = sin(\alpha\pi x^3)+z_i, \;\; \text{where}\;z_i \sim\mathcal{N}(0,\sigma^2),$$

for $i=1, \cdots, 400$, $\alpha=4$, and $\sigma=0.2$.



```{r}
set.seed(42)
n <- 400
x <- runif(n)
y <- sin(4 * pi * x^3) + rnorm(n, 0, 0.2)
df <- data.frame(x = x, y = y)
```

Estimate the conditional expectation $\mu(x)=\mathbb{E}(y|x)$ using a kernel regression smoother
$$\hat{\mu}(x)=\frac{\sum_{i=1}^nk_{\lambda}(x,x_i)y_i}{\sum_{i=1}^nk_{\lambda}(x,x_i)},$$
where $\lambda>0$. The estimation is produced by using a Gaussian kernel with variance $\lambda^2$.

Let's first implement the function in `R`:
```{r}
meanKRS <- function(y, x, x0, lam) {
  n <- length(x)
  n0 <- length(x0)
  out <- numeric(n0)

  # compute the estimated conditional expectation
  for (i in 1:n0) {
    kernel <- dnorm(x, x0[i], lam)
    out[i] <- sum(kernel * y) / sum(kernel)
  }

  return(out)
}
```

Use the estimator to produce two fits, for two different $\lambda$ values
```{r}
# test points
x_new <- seq(0, 1, length.out = 1000)

# fit estimator for large lambda
muSmoothLarge <- meanKRS(y = y, x = x, x0 = x_new, lam = 0.06)

# fit estimator for small lambda
muSmoothSmall <- meanKRS(y = y, x = x, x0 = x_new, lam = 0.02)
```

Plot data and the two fits for the different lambdas.
```{r echo=FALSE} 
df <- data.frame(df)
df_new <- data.frame(x_new, muSmoothLarge, muSmoothSmall)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLarge, colour = "large"), size = 1) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmall, colour = "small"), size = 1) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1")) +
  labs(title = "Fitted smooths using R implementation")
```

### Q1a

A `C` implementation of the `meanKRS` function.

```{Rcpp eval = FALSE}
// include required headers
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export(name = "meanKRSRcpp")]]
NumericVector meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  
  // define required variables and output vector
	int n = x.size();
	int n0 = x0.size();
	NumericVector out(n0);
	
	// estimate conditional expectation
	for(int i = 1; i < n0; i++){
	  NumericVector kernel = dnorm(x, x0[i],lambda,0);
	  
	  // numerator
	  double num = 0;
	  for(int j = 1; j < n; j++){
	    num = num + kernel[j] * y[j];
	  }
	  
	  //denominator
	  double denom = 0;
	  for(int j = 1; j < n; j++){
	    denom = denom + kernel[j];
	  }
	  
  out[i] = num / denom;
	}
	
	return out;
}
```

Source the file containing the function above.

```{r}
getwd()
sourceCpp("../code/meanKRS.cpp")
```

Fit the data using the `C` function.

```{r}
muSmoothLargeRcpp <- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)
muSmoothSmallRcpp <- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.02)
```

Plot the two fits

```{r echo = FALSE}
df_new <- data.frame(x_new, muSmoothLargeRcpp, muSmoothSmallRcpp)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLargeRcpp, colour = "large"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmallRcpp, colour = "small"), size = 0.8) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1")) +
  labs(title = "Fitted smooths using C implementation")
```

Check whether the two function give the same results. If the error is roughly $10^{-15}$, then the error is mainly due to machine precision.

```{r}
max(abs(meanKRS(y = y, x = x, x0 = x_new, lam = 0.06) - meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)))
```

Compare the efficiency of the two functions and note that the `Rcpp` implementation is about $1.5$ times faster.

```{r message=FALSE}
microbenchmark(meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), meanKRS(y = y, x = x, x0 = x_new, lam = 0.06), unit = "relative")
```

### Q1b

Since choosing $\lambda$ requires a more formal way to do so, we implement a cross validation function to use.

```{r}
cv_error <- function(y, x, lambdas, k = 10) {
  # define error metric
  mse <- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors <- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds <- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error <- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test <- x[which(folds == fold)]
      y_test <- y[which(folds == fold)]
      x_train <- x[which(folds != fold)]
      y_train <- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit <- meanKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error <- error + (mse(fit, y_test) / k)
    }
    errors[i] <- error
  }
  return(errors)
}
```

Find the lambda which has the smallest cross-validation error.
```{r}
lambdas <- seq(0.001, 0.025, length.out = 50)
errors_krs <- cv_error(y, x, lambdas, 10)
# lambda_min_krs <- lambdas[which.min(cv)]
```


Plot the cross validation error curve and the lambda that gives the smallest error.
```{r}
data.frame(lambda = lambdas, error = errors_krs) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21,size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

`C` implementation of cross validation.

```{rcpp eval = FALSE}
// mean squared error
double mse(const NumericVector a, const NumericVector b){
  return mean(pow((a-b),2));
}

// random wrapper
int randWrapper(const int n){
  return(floor(unif_rand()));
}

// create a sequence of n samples from 0 to k
// [[Rcpp::export(name = "sample_rcpp")]]
NumericVector sample_rcpp(int k, int n){
  NumericVector seq(n);

  // create ordered sequence
  for(int i = 1; i <= k; i++){
    for(int j = i; j <= n; j= j + k){
      seq[j-1] = i-1;
    }
  }

  // randomly shuffle the sequence of indeces
  std::random_shuffle(seq.begin(), seq.end(), randWrapper);
  return seq;
}

// cross validation function

// [[Rcpp::export(name = "cv_meanKRS_cpp")]]
NumericVector cv_meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector lambdas, const int k){
  int n = x.size();
  int m = lambdas.size();

  NumericVector errors(m);
  NumericVector folds = sample_rcpp(k, n);

  // iterate over lambdas
  for(int i=0; i < m; i++){
    double error = 0;

    // iterate over folds
    for(int j = 0; j < k; j++){

      // split the data into train and test set
      NumericVector x_test = x[(folds==j)];
      NumericVector x_train = x[(folds!=j)];
      NumericVector y_test = y[(folds==j)];
      NumericVector y_train = y[(folds!=j)];

      // fit model on training data and get
      NumericVector fit = meanKRSR_cpp(y_train, x_train, x_test, lambdas[i]);

      // calculate error for fold
      error = error + mse(fit, y_test)/k;
    }
    errors[i] = error;
  }

  return errors;
}
```

Source the `C` file.

```{r}
sourceCpp("../code/cv_meanKRS.cpp")
```

Use the `C` implementation to perform the cross validation.
```{r}
errors_krs_cpp <- cv_meanKRS_cpp(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r}
data.frame(lambda = lambdas, error = errors_krs_cpp) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

Compare the efficiency of the `R` and `C` implementations.

```{r eval=FALSE}
microbenchmark(cv_meanKRS_cpp(y, x, lambdas, 5), cv_error(y, x, lambdas, 5), unit = "relative")
```

###Q2

Implement the KRS with a variable $\lambda$ value.

```{r}
varKRS <- function(y, x, x0, lam) {
  n <- length(x)
  n0 <- length(x0)
  mu <- numeric(n)

  out <- numeric(n0)
  madHat <- numeric(n0)

  for (i in 1:n) {
    kernel <- dnorm(x, x[i], lam)
    mu[i] <- sum(kernel * y) / sum(kernel)
  }

  resAbs <- abs(y - mu)
  for (ii in 1:n0) {
    kernel <- dnorm(x, x0[ii], lam)
    madHat[ii] <- sum(kernel * resAbs) / sum(kernel)
  }

  w <- 1 / madHat
  w <- w / mean(w)

  for (i in 1:n0) {
    kernel <- dnorm(x, x0[i], lam * w[i])
    out[i] <- sum(kernel * y) /
      sum(kernel)
  }

  return(out)
}
```

Fit the adaptive smooth using the `R` implementation.

```{r}
muSmoothAdapt <- varKRS(y = y, x = x, x0 = x_new, lam = 0.06)
```

Plot the fit of all smooths.

```{r}
df_new <- data.frame(x_new, muSmoothLarge, muSmoothSmall, muSmoothAdapt)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLarge, colour = "large"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmall, colour = "small"), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothAdapt, colour = "adapt"), size = 0.8) +
  scale_color_manual(name = "Lambda", values = c(large = "#ffd400", small = "#19c6c1", adapt = "#ef3939")) +
  labs(title = "Fitted smooths using R implementation")
```

Writing the function in `C`
```{rcpp eval=FALSE}
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export(name = "varKRS_cpp")]]
NumericVector varKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  int n = x.size();
  int n0 = x0.size();
  NumericVector res(n);
  NumericVector mu(n);

  NumericVector madHat(n0);
  NumericVector out(n0);

  for(int i = 0; i < n; i++){
    NumericVector kernel = dnorm(x, x[i],lambda,0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    mu[i] = num / denom;
  }

  NumericVector resAbs(n);
  resAbs = abs(y - mu);


  NumericVector w(n0);
  double sum_w = 0;

  for(int i=0; i < n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda,0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * resAbs[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    madHat[i] = num / denom;
    w[i] = denom / num;
    sum_w = sum_w + w[i];

  }

  for(int i = 0; i < n0; i++){
    w[i] = n0 * w[i] / sum_w;
  }

  for(int i=0; i < n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda*w[i],0);

    double num = 0;
    for(int j = 0; j < n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j < n; j++){
      denom = denom + kernel[j];
    }
    out[i] = num / denom;
  }


  return out;
}
```

Source the `C` file for the above function.

```{r}
sourceCpp("../code/varKRS.cpp")
```

Fit the model.
```{r}
muSmoothAdapt_rcpp <- varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)
```

Check that it returns the same fitted values as the `R` implementation.
```{r}
max(abs(muSmoothAdapt - muSmoothAdapt_rcpp))
```

Comparing the performances it is clear that the `C` implementation is faster.
```{r}
microbenchmark(varKRS(y = y, x = x, x0 = x_new, lam = 0.06), varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), unit = "relative")
```

Perform cross validation using `R`
```{r}
cv_error_var <- function(y, x, lambdas, k = 10) {
  # define error metric
  mse <- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors <- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds <- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error <- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test <- x[which(folds == fold)]
      y_test <- y[which(folds == fold)]
      x_train <- x[which(folds != fold)]
      y_train <- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit <- varKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error <- error + (mse(fit, y_test) / k)
    }
    errors[i] <- error
  }
  return(errors)
}
```

Compute the errors
```{r}
errors_var <- cv_error_var(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r warning=FALSE, message=FALSE}
data.frame(lambda = lambdas, error = errors_var) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5)
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey",linetype='dashed')
```

Comparing the performance of the `R` and the `C` implementation of the cross validation we get

```{r}
sourceCpp("../code/cv_varKRS.cpp")
```

Compute the errors.
```{r}
errors_var_cpp <- cv_varKRS_cpp(y, x, lambdas, 10)
```

Plot the cross validation error curve.
```{r warning=FALSE, message=FALSE}
data.frame(lambda = lambdas, error = errors_var_cpp) %>%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = "darkgrey", linetype='dashed')
```

```{r eval=FALSE}
microbenchmark(cv_varKRS_cpp(y, x, lambdas, 5), cv_error_var(y, x, lambdas, 5), unit = "relative")
```
