---
title: Advanced Rcpp
author: Andrea Becsek
date: '2020-06-13'
slug: advanced-rcpp
categories: []
tags: []
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, echo=FALSE}
library(ggplot2)
library(tidyverse)
library(viridis)
library(Rcpp)
library(microbenchmark)
library(RcppArmadillo)
library(gridExtra)
library(mvtnorm)
```


Load dataset on solar electricity production in Sydney
```{r}
set.seed(123)
# load dataset
solar <- readRDS('../data/solarAU.RDS')

# reduce dataset to 2000 observations
solar <- solar[1:2000,]

# create a log production variable
solar$logprod <- log(solar$prod + 0.01)

head(solar)
```

Plot the log production as a function of `toy` and `tod`
```{r}
ggplot(solar, aes(x=toy, y=tod,z=logprod))+
  stat_summary_hex()+
  scale_fill_gradientn(colours=viridis(50))
```

To model the log-production we start with a simply polynomial regression model
$$\mathbb{E}(y|\boldsymbol{x})=\beta_0+\beta_1tod+\beta_2tod^2+\beta_3toy+\beta_4toy^2$$

Fitting this model in `R`
```{r}
fit <- lm(logprod ~tod+I(tod^2)+toy+I(toy^2), data=solar)
lm_r <- fit$coefficients
```

Fit the same model using `Rcpp`. First set the model matrix and output variable is
```{r}
X <- with(solar,cbind(1,tod,tod^2,toy,toy^2))
y <- solar$logprod
```

Using cholesky factorisation to solve the linear system
$$(X^TX)\beta=X^Ty$$
Note that $X^TX$ is a symetric positive-definite square matrix, so it can be decomposed as $A=X^TX=U^TU$, where $U$ is a upper triangular matrix. 

```{r}
lm_naive_r <- function(X,y){
  beta <- solve(t(X)%*%X)%*%t(X)%*%y
  return(beta)
}
naive_r <- lm_naive_r(X,y)

lm_chol_r <- function(X,y){
  U <- chol(t(X)%*%X)
  C <- forwardsolve(t(U), t(X)%*%y)
  beta <- solve(U,C)
  return(beta)
}
chol_r <- lm_chol_r(X,y)

lm_qr_r <- function(X,y){
  qr <- qr.default(X)
  beta <- backsolve(qr$qr, qr.qty(qr,y))
  return(beta)
}
qr_r <- lm_qr_r(X,y)

microbenchmark(lm(logprod ~tod+I(tod^2)+toy+I(toy^2), data=solar),lm_naive_r(X,y), lm_chol_r(X,y), unit='relative')
```

Check that the obtained estimates are the same as those given by `lm`
```{r}
max(abs(lm_r-naive_r))
max(abs(lm_r-chol_r))
max(abs(lm_r-qr_r))
```

Writing a similar cholesky implementation using `Rcpp`

```{r}
sourceCpp(code = '
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>
using namespace arma;

// [[Rcpp::export(name = "lm_chol_Rcpp")]]
arma::vec lm_chol_Rcpp(arma::mat& X, arma::vec& y) {
  arma::mat U = chol(X.t() * X);
  arma::vec C = solve(trimatl(U.t()), X.t() * y);
  arma::vec beta = solve(trimatu(U),C);
  return beta;
}')
```

<!-- # ```{r} -->
<!-- # sourceCpp(code = ' -->
<!-- # // [[Rcpp::depends(RcppArmadillo)]] -->
<!-- # #include <RcppArmadillo.h> -->
<!-- # using namespace arma; -->
<!-- #  -->
<!-- # // [[Rcpp::export(name = "lm_qr_Rcpp")]] -->
<!-- # arma::mat lm_qr_Rcpp(arma::mat& X, arma::vec& y) { -->
<!-- #   arma::mat Q, R; -->
<!-- #   qr_econ(Q,R,X); -->
<!-- #   arma::vec beta = solve(R, Q.t()*y); -->
<!-- #   return beta; -->
<!-- # }') -->
<!-- # ``` -->


Check that the function gives the correct result.
```{r}
chol_Rcpp <- lm_chol_Rcpp(X,y)
max(abs(lm_r-chol_Rcpp))
```
Since the error is comparable to machine precision, the function seems to be working correctly.

Compare the `r` and the `RcppArmadillo` implementation. The `r` implementation is $5$ times slower than the `RcppArmadillo` one.
```{r}
microbenchmark(lm_chol_r(X,y),lm_chol_Rcpp(X,y), unit='relative')
```

To understand whether the polynomial fit is a good model choice, we note that the second plot captures a non-linearity of the residuals.
```{r}
solar$fitPoly <- fit$fitted.values

pl1 <- ggplot(solar,
              aes(x = toy, y = tod, z = fitPoly)) +
       stat_summary_hex() +
       scale_fill_gradientn(colours = viridis(50))

pl2 <- ggplot(solar,
              aes(x = toy, y = tod, z = logprod - fitPoly)) +
       stat_summary_hex() +
       scale_fill_gradientn(colours = viridis(50))
grid.arrange(pl1, pl2, ncol = 2)
```

To obtain a better fit, we implement a local least regression approach which enables the coefficients to depend on $\boldsymbol{x}$. To achieve this, for a fixed value of $\boldsymbol{x}_0$ we find $\hat{\beta}(\boldsymbol{x}_0)$ that minimizes the objective:
$$\hat{\beta}(\boldsymbol{x}_0)=\underset{\beta}{\text{argmin}}\sum_{i=1}^nk_{\boldsymbol{H}}(\boldsymbol{x}_0-\boldsymbol{x}_i)(y_i-\tilde{\boldsymbol{x}_i}^T\beta)^2,$$
where $k_{\boldsymbol{H}}$ is a density kernel with positive bandwidth matrix $\boldsymbol{H}$. 

Implementing this model in `r` using a Gaussian kernel:
```{r}
lmLocal <- function(y, x0, X0, x, X, H){
  w <- dmvnorm(x, x0, H)
  fit <- lm(y ~ -1 + X, weights = w)
  return( t(X0) %*% coef(fit) )
}
```

Since this approach requires re-estimating the model for every $\boldsymbol{x}_0$, we need to fit `r nrow(solar)` models, which might take a while. So we start by only using a random subsample of $2000$ data points.
```{r}
subsample <- sample(1:nrow(solar), 2000, replace = FALSE)
y <- solar$logprod
set.seed(123)
solar_sub <- solar[subsample,]
x <- as.matrix(solar[c('tod','toy')])
x0 <- x[subsample,]
X0 <- X[subsample,]
H = diag(c(1, 0.1)^2)
```

The predictions at each of the $2000$ random locations are given by
```{r}
predLocal_r <- sapply(1:2000, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H  = H)
})
```

Let's check the fit of this model
```{r}
solar_sub$fitLocal_r <- predLocal_r

pl1 <- ggplot(solar_sub,
       aes(x = toy, y = tod, z = fitLocal_r)) +
       stat_summary_hex() +
       scale_fill_gradientn(colours = viridis(50))

pl2 <- ggplot(solar_sub,
       aes(x = toy, y = tod, z = logprod - fitLocal_r)) +
       stat_summary_hex() +
       scale_fill_gradientn(colours = viridis(50))

grid.arrange(pl1, pl2, ncol = 2)
```

The residual plot shows no clear pattern, so this fit is definitely better. Also, the plot on the left does look similar to the data, which is also a good indicator.

Let's now implement this model using `RcppArmadillo`. First, we call the `Armadillo` function that evaluates the Gaussian log-density, `dmvnInt`. Note that this function takes matrix of quantiles $X$, mean vector $mu$ and the lower triangular Cholesky factor of the covariance matrix $L$.
```{r}
sourceCpp(code = '
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>
using namespace arma;


// [[Rcpp::export(name = "dmvnInt")]]
vec dmvnInt(mat & X, const rowvec & mu, mat & L)
{
  unsigned int d = X.n_cols;
  unsigned int m = X.n_rows;
  
  vec D = L.diag();
  vec out(m);
  vec z(d);
  
  double acc;
  unsigned int icol, irow, ii;
  for(icol = 0; icol < m; icol++)
  {
    for(irow = 0; irow < d; irow++)
    {
     acc = 0.0;
     for(ii = 0; ii < irow; ii++) acc += z.at(ii) * L.at(irow, ii);
     z.at(irow) = ( X.at(icol, irow) - mu.at(irow) - acc ) / D.at(irow);
    }
    out.at(icol) = sum(square(z));
  }

  out = exp( - 0.5 * out - ( (d / 2.0) * log(2.0 * M_PI) + sum(log(D)) ) );

  return out;
}')
```

Implementation of local polynomial regression

```{r}
Rcpp::sourceCpp('../code/local_rcpp.cpp')
```

```{r}
predLocal_Rcpp <- lm_local_Rcpp(y,x0,X0,x,X,H)
```

Compare results of the two functions
```{r}
max(predLocal_r-predLocal_Rcpp)
```

Compare performance of the two implementations.

```{r}
system.time(sapply(1:2000, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H  = H)
}))
system.time(lm_local_Rcpp(y,x0,X0,x,X,H))
```


To perform weighted least squares, we simply define $\tilde{X}=\sqrt{w}X$ and $\tilde{y}=\sqrt{w}y$ and fit the linear model as before.

<!-- ```{r} -->
<!-- lm(y~X-1, weights = w) -->
<!-- X_tilde <- sqrt(w)*X -->
<!-- y_tilde <- sqrt(w)*y -->
<!-- lm(y_tilde~X_tilde-1) -->
<!-- lm_chol_r(X_tilde,y_tilde) -->
<!-- ``` -->



