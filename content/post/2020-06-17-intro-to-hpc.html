---
title: Intro to HPC
author: Andrea Becsek
date: '2020-06-17'
slug: intro-to-hpc
categories: []
tags:
  - hpc
  - linux
  - bash
---



<p>High performance computing is a way to process data much faster than a single computer possibly could. This is achieved by parallelizing code across a cluster of computer and nodes.</p>
<div id="logging-in" class="section level2">
<h2>Logging in</h2>
<div id="setting-vpn-if-not-on-university-premises" class="section level4">
<h4>Setting VPN (if not on university premises)</h4>
<p>If you are not on university premises you will have to set up the University VPN. Setting up a connection on Linux can be done as follows:</p>
<ol style="list-style-type: decimal">
<li>Go to <a href="https://uobnet2.bristol.ac.uk" class="uri">https://uobnet2.bristol.ac.uk</a> and sign in.</li>
<li>If this is the first time you do this on your computer then you need to install F5 Network Acess. There will be a link to download it on the website.</li>
<li>If you have already done step 2. before then you will be shown a prompt to launch the application.</li>
</ol>
<p>Once you are connected you should see this window</p>
<p><img src="/post/2020-06-17-intro-to-hpc_files/vpn.png" /></p>
</div>
<div id="log-in-using-ssh" class="section level4">
<h4>Log in using ssh</h4>
<p>Now you are ready to log into the cluster using <code>ssh</code>. On Linux you simply use the terminal and type the following</p>
<pre class="bash"><code>ssh -X ky19161@bluecrystalp3.acrc.bris.ac.uk</code></pre>
<p>Note: the <code>-X</code> flag allows us to run graphical applications remotely but for them to be displayed locally.</p>
</div>
<div id="copy-files-to-and-from-cluster" class="section level4">
<h4>Copy files to and from cluster</h4>
<p>At some point you will most definitely need to transfer data or source code from your computer to the cluster. Note that the cluster is not intended for long-term file storage, so anything that you leave there has no guarantee that it will remain there. For that reason make sure you always transfer your results back to your local machine. For storage purposes you could instead visit <a href="http://www.bristol.ac.uk/acrc/research-data-storage-facility/" class="uri">http://www.bristol.ac.uk/acrc/research-data-storage-facility/</a>.</p>
<p>We use <code>scp</code>, which stands for <code>secure copy protocol</code>, and we always do this from within our local machine</p>
<ul>
<li><p>copy from local to remote:
<code>scp data.r ky19161@bluecrystalp3.acrc.bris.ac.uk:~</code></p></li>
<li><p>copy from remote to local:
<code>scp ky19161@bluecrystalp3.acrc.bris.ac.uk:data.r .</code></p></li>
</ul>
<p>To check you file storage quota you can use: <code>pan_quota</code> and it shows the result in bytes.</p>
<p><strong>Advice for file usage</strong>: using a few larger files helps the job run faster than many small ones. This is because locating files on the cluster takes quite a bit of time.</p>
</div>
</div>
<div id="environment-modules" class="section level2">
<h2>Environment Modules</h2>
<p>BlueCrystal has an environment modules package which enables one to customize their environment. What is perhaps the most important is that you must always make sure that the module is loaded. The main commands to do this are:</p>
<ul>
<li><code>module avail</code>: list available modules</li>
<li><code>module list</code>: list loaded modules</li>
<li><code>module add module_name</code>: load module</li>
<li><code>module del</code>: remove module</li>
<li><code>module whatis</code>: details about the module</li>
</ul>
<p>It is recommended that you include the required modules in your shell startup file <code>.bashrc</code>. However, make sure you know what you are doing beforehand and don’t modify anything else.</p>
<p>For example, if you want to use <code>R</code> and <code>Python</code> your <code>.bashrc</code> should inclue</p>
<pre class="bash"><code>module add languages/R-3.0.2
module add languages/python-3.3.2</code></pre>
</div>
<div id="how-to-run-a-job" class="section level2">
<h2>How to run a job</h2>
<ol style="list-style-type: decimal">
<li>Include the required modules in your <code>.bashrc</code></li>
<li>Compile code if needed</li>
<li>Copy scripts or data to the cluster.</li>
<li>Create job submission script</li>
<li>Submit job script to queuing system.</li>
</ol>
<div id="queuing-system" class="section level4">
<h4>Queuing system</h4>
<ul>
<li><code>qstat -q</code>: see all the queues</li>
<li><code>qstat job_number</code>: check status of job</li>
<li><code>watch qstat job_id</code>: watch status of your job</li>
<li><code>qstat username</code>: see all jobs from a user</li>
</ul>
</div>
<div id="submission-script" class="section level4">
<h4>Submission script</h4>
<p><strong>Single-thread job</strong>: runs a program using a single processor</p>
<pre class="bash"><code>#!/bin/bash
# resources
#PBS -l nodes=1:ppn=1
#PBS -l walltime=02:00:00
# on compute node, change directory to &#39;submission directory&#39;:
cd $PBS_O_WORKDIR
# time the program:
time ./my-serial-program</code></pre>
<ul>
<li><code>#!/bin/bash</code> is called a shebang and it is used to say that the script is a shell script</li>
<li><code>PBS -l nodes=1:ppn=1</code>: tells the computer to use a single node for the job. Note that all <code>#PBS</code> commands are for the queuing system.</li>
<li><code>#PBS -l walltime=02:00:00</code>: how long we estimate the job to take (2 hours in this case). It is always better to overestimate, rather than underestimate.</li>
<li><code>cd $PBS_O_WORKDIR</code>: ensures that the job is run from the submission directory</li>
<li><code>time ./my-serial-program</code>: times how long the job took to run</li>
</ul>
<p>To submit the job, simply type:
<code>qsub submission_script</code>.</p>
<p><strong>Note</strong>: if a permission error occurs you might have to run `chmod u+x filename’ to make the file executable for your user.</p>
</div>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>We have the following script that we want to run.</p>
<pre class="bash"><code>#!/bin/bash

for text_file in Alices_Adventures_in_Wonderland.txt Metamorphosis.txt The_Impo\
rtance_of_Being_Earnest.txt The_Picture_of_Dorian_Gray.txt
do
        echo $text_file
        file_name=&quot;${text_file%.*}_count.txt&quot;
        echo $file_name
        cat $text_file  |tr -d &#39;[:punct:]&#39;|tr &quot; &quot; &quot;\n&quot; | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39;| sort |
                uniq -c|sort -n | fgrep -vwf stop_words.txt | tail -n 15 &gt;&gt; $file_name

done</code></pre>
<p>This is a similar script to the one in the Linux portfolio. It takes the specified text files and returns the top most common words in different files, except for the specified stop words (for, at, a etc).</p>
<p>To run this, we need to upload it to the remote directory together with the stop words file and the text files.
<img src="/post/2020-06-17-intro-to-hpc_files/scp.png" /></p>
<p>Next we log into the server using ssh and note that all the required files are now in the book directory.
<img src="/post/2020-06-17-intro-to-hpc_files/ssh.png" /></p>
<p>Moreover, we have created a submission script. As we see, the job is set to run the script on a single node, and sets the estimated run time to 10 minutes. Moreover, we set the working directory to <code>books</code> and we use that to obtain and create our files.</p>
<pre class="bash"><code>#!/bin/bash

#PBS -l nodes=1:ppn=1,walltime=00:10:00

# Define working directory 
export WORK_DIR=$HOME/books

# Define executable
export EXE=/bin/hostname

# Change into working directory 
cd $WORK_DIR

# Print job id and working directory

echo JOB ID: $PBS_JOBID

echo Working Directory: `pwd`

# Execute code 
$EXE</code></pre>
<p>We submit the job, and check its status. The status can be seen under the <code>S</code> column as <code>Q</code> for Queued, <code>R</code> for Running and <code>C</code> for Completed. Once the job finishes we obtain the new files that contain the top word counts and two additional files.
<img src="/post/2020-06-17-intro-to-hpc_files/qstat_done.png" /></p>
<p>The <code>.e</code> file contains the running time in this case. But it can contain errors.
<img src="/post/2020-06-17-intro-to-hpc_files/efile.png" /></p>
<p>The <code>.o</code> file has the outputs that we have printed as part of out submission script (workind directory and node) and the actual code (input and output file names).
<img src="/post/2020-06-17-intro-to-hpc_files/ofile.png" /></p>
<p>While we have only examined how to submit a serial job, the HPC facilities are perhaps even more usefull for parallel jobs.</p>
</div>
