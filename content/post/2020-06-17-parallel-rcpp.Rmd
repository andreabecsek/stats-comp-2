---
title: Parallel Rcpp
author: ''
date: '2020-06-17'
slug: parallel-rcpp
categories: []
tags: []
---

## Rcpp Parallel
```{r echo=FALSE}
library(Rcpp)
library(microbenchmark)
```


Consider a simple scenario. We want to flip $100$ coins and get the results.
We can do this in R, in what is an already optimized way.
```{r}
rbinom(100, 1, 0.5)
```


Or we can do this using
```{Rcpp eval=FALSE}
#include <iostream>
#include <Rcpp.h>
using namespace Rcpp;

//[[Rcpp::export(flips)]]
NumericVector coin()
{
  NumericVector out(100);
  for(i=0; i<100; i++)
  {
    out[i] = rand() % 2;
  }
  return mean(out);
}
```

If we source and run the code it seems to be working fine
```{r}
sourceCpp("../code/coin_rcpp.cpp")
flips()
```

Moreover, comparing it to the R version it actually takes a lot more time. This is because the `binom` function is already really well optimized. But for the sake of this demonstration we carry on and see what happens.
```{r}
microbenchmark(rbinom(100, 1, 0.5),
               flips(),
               times = 10,
               unit='relative')
```

Let's explore whether this could be improved upon by running the code in parallel. We do this using `OpenMP` and `RcppParallel`. To do so there are a few changes we need to make to the code:

* include the `RcppParallel` header
* include the dependency of `openmp` and `RcppParallel`
* define the input and output via special wrappers that are thread-safe

The actual code looks like this
```{Rcpp eval=FALSE}
#include <iostream>
#include <Rcpp.h>
#include <RcppParallel.h>
using namespace Rcpp;

// [[Rcpp::plugins(openmp)]]
// [[Rcpp::depends(RcppParallel)]]

//[[Rcpp::export(flips_par)]]
NumericVector coin(int ncores)
{
  NumericVector out(100);
  RcppParallel::RVector<double> vo(out);
  
#if defined(_OPENMP)
  #pragma omp parallel for num_threads(ncores)
#endif
  
  for(int i=0; i<100; i++)
  {
    vo[i] = rand() % 2;
  }
  return out;
}

```

Checking that the code does indeed work
```{r}
sourceCpp("../code/coin_parallel.cpp")
flips_par(3)
```

Let's see how using a different number of cores changes the performance. Clearly the more cores we add, the more time it takes for the computation to be performed. This is a common mistake that can be made if we are overly keen to parallelize an operation without further thought. What happens is that the operation itself takes way less time than the parallelization process.
```{r}
microbenchmark(rbinom(100, 1, 0.5),
               flips_par(1),
               flips_par(3),
               flips_par(6),
               flips_par(8),
               times = 10,
               unit = 'relative')
```

<!-- To understand more exactly what parallelization involves, let's take a look at the actual function that is used. Note how it takes the beginning and end of a for loop, the worker and the number of minimum number of iterations to be performed by a thread. -->
<!-- ```{Rcpp eval=FALSE} -->
<!-- void parallelFor(std::size_t begin, -->
<!--                  std::size_t end,  -->
<!--                  Worker& worker, -->
<!--                  std::size_t grainSize = 1) -->
<!-- ``` -->

<!-- ## Reductions -->

<!-- Let's create a reduction that finds the number of prime numbers in a sequence. -->
<!-- ```{r} -->
<!-- sourceCpp("../code/is_prime.cpp") -->
<!-- ``` -->



