<!doctype html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Andrea Becsek">
    <meta name="description" content="Portfolio for Statistical Computing 2.">
    <meta name="keywords" content="">
    <meta name="generator" content="Hugo 0.71.0" />
    <title> Rcpp | Statistical Computing 2 Portfolio</title>
    <meta name="description" content="Rcpp - Portfolio for Statistical Computing 2.">
    <meta itemprop="name" content="Rcpp">
    <meta itemprop="description" content="Rcpp - Portfolio for Statistical Computing 2.">
    <meta property="og:title" content="Rcpp">
    <meta property="og:description" content="Rcpp - Portfolio for Statistical Computing 2.">
    <meta property="og:image" content="https://www.gravatar.com/avatar/4b543f7324877bb626c1c0ed53277844?size=200">
    <meta property="og:url" content="/post/rcpp/">
    <meta property="og:site_name" content="Statistical Computing 2 Portfolio">
    <meta property="og:type" content="article">

    


    
    <link href="/post/rcpp/" rel="alternate" type="application/rss+xml" title="Statistical Computing 2 Portfolio" />
    <link href="/post/rcpp/" rel="feed" type="application/rss+xml" title="Statistical Computing 2 Portfolio" />
    

    

    <link rel="stylesheet" href="/theme.css">

    

    
    
    
</head>

<body class="bilberry-hugo-theme">

    
    <nav>

    <div class="container">
        <ul class="topnav">
            
            
            <li><a href="/page/about-bilberry/">About Bilberry</a></li>
            
            
            
            <li><a href="https://github.com/Lednerb/bilberry-hugo-theme" target="_blank">Github</a></li>
            
            
        </ul>

        
    </div>
</nav>


    
<header>

    <div class="container">
        <div class="logo">
            <a id="siteBaseUrl" href="/" class="logo">
                
                <img src="https://www.gravatar.com/avatar/4b543f7324877bb626c1c0ed53277844?d=mm&size=200" alt="">
                

                <span class="overlay"><i class="fa fa-home"></i></span>
            </a>
        </div>
        <div class="titles">
            <h3 class="title"><a
                    href="/">Statistical Computing 2 Portfolio</a></h3>
            
            <span class="subtitle">Andrea Becsek</span>
            
        </div>

        

        
            <div class="toggler">
                
                <i class="fa fa-bars" aria-hidden="true"></i>
            </div>
        </div>
</header>


    <div class="main container">
        
     
    <div class="article-wrapper u-cf single">
        
            <a class="bubble" href="/post/rcpp/">
    <i class="fas fa-fw fa-pencil-alt"></i>
</a>

<article class="default article">
    

    <div class="content">
    <h1 class="article-title"><a href="/post/rcpp/">Rcpp</a></h1>
    <div class="meta">
        
        
        <span class="date moment">2020-05-25</span>
        
        

        

        

        
        <span class="author">
            
            
            <a href="/author/andrea-becsek/">Andrea Becsek</a>
            
        </span>
        
    </div>

    
    


<p>The aim of this portfolio is to demonstrate the usage of <code>C++</code> in conjunction with <code>R</code> via the <code>Rcpp</code> package. The application that we are considering is the adaptive kernel regression smoothing.</p>
<p>Simulate data generated by the model
<span class="math display">\[y_i = sin(\alpha\pi x^3)+z_i, \;\; \text{where}\;z_i \sim\mathcal{N}(0,\sigma^2),\]</span></p>
<p>for <span class="math inline">\(i=1, \cdots, 400\)</span>, <span class="math inline">\(\alpha=4\)</span>, and <span class="math inline">\(\sigma=0.2\)</span>.</p>
<pre class="r"><code>set.seed(42)
n &lt;- 400
x &lt;- runif(n)
y &lt;- sin(4 * pi * x^3) + rnorm(n, 0, 0.2)
df &lt;- data.frame(x = x, y = y)</code></pre>
<p>Estimate the conditional expectation <span class="math inline">\(\mu(x)=\mathbb{E}(y|x)\)</span> using a kernel regression smoother
<span class="math display">\[\hat{\mu}(x)=\frac{\sum_{i=1}^nk_{\lambda}(x,x_i)y_i}{\sum_{i=1}^nk_{\lambda}(x,x_i)},\]</span>
where <span class="math inline">\(\lambda&gt;0\)</span>. The estimation is produced by using a Gaussian kernel with variance <span class="math inline">\(\lambda^2\)</span>.</p>
<p>Letâ€™s first implement the function in <code>R</code>:</p>
<pre class="r"><code>meanKRS &lt;- function(y, x, x0, lam) {
  n &lt;- length(x)
  n0 &lt;- length(x0)
  out &lt;- numeric(n0)

  # compute the estimated conditional expectation
  for (i in 1:n0) {
    kernel &lt;- dnorm(x, x0[i], lam)
    out[i] &lt;- sum(kernel * y) / sum(kernel)
  }

  return(out)
}</code></pre>
<p>Use the estimator to produce two fits, for two different <span class="math inline">\(\lambda\)</span> values</p>
<pre class="r"><code># test points
x_new &lt;- seq(0, 1, length.out = 1000)

# fit estimator for large lambda
muSmoothLarge &lt;- meanKRS(y = y, x = x, x0 = x_new, lam = 0.06)

# fit estimator for small lambda
muSmoothSmall &lt;- meanKRS(y = y, x = x, x0 = x_new, lam = 0.02)</code></pre>
<p>Plot data and the two fits for the different lambdas.
<img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div id="q1a" class="section level3">
<h3>Q1a</h3>
<p>A <code>C</code> implementation of the <code>meanKRS</code> function.</p>
<pre class="cpp"><code>// include required headers
#include &lt;Rcpp.h&gt;
#include &lt;Rmath.h&gt;
using namespace Rcpp;

// [[Rcpp::export(name = &quot;meanKRSRcpp&quot;)]]
NumericVector meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  
  // define required variables and output vector
    int n = x.size();
    int n0 = x0.size();
    NumericVector out(n0);
    
    // estimate conditional expectation
    for(int i = 1; i &lt; n0; i++){
      NumericVector kernel = dnorm(x, x0[i],lambda,0);
      
      // numerator
      double num = 0;
      for(int j = 1; j &lt; n; j++){
        num = num + kernel[j] * y[j];
      }
      
      //denominator
      double denom = 0;
      for(int j = 1; j &lt; n; j++){
        denom = denom + kernel[j];
      }
      
  out[i] = num / denom;
    }
    
    return out;
}</code></pre>
<p>Source the file containing the function above.</p>
<pre class="r"><code>sourceCpp(&quot;../code/meanKRS.cpp&quot;)</code></pre>
<p>Fit the data using the <code>C</code> function.</p>
<pre class="r"><code>muSmoothLargeRcpp &lt;- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)
muSmoothSmallRcpp &lt;- meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.02)</code></pre>
<p>Plot the two fits</p>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Check whether the two function give the same results. If the error is roughly <span class="math inline">\(10^{-15}\)</span>, then the error is mainly due to machine precision.</p>
<pre class="r"><code>max(abs(meanKRS(y = y, x = x, x0 = x_new, lam = 0.06) - meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)))</code></pre>
<pre><code>## [1] 2.109424e-15</code></pre>
<p>Compare the efficiency of the two functions and note that the <code>Rcpp</code> implementation is about <span class="math inline">\(1.5\)</span> times faster.</p>
<pre class="r"><code>microbenchmark(meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), meanKRS(y = y, x = x, x0 = x_new, lam = 0.06), unit = &quot;relative&quot;)</code></pre>
<pre><code>## Unit: relative
##                                               expr      min       lq     mean
##  meanKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06) 1.000000 1.000000 1.000000
##      meanKRS(y = y, x = x, x0 = x_new, lam = 0.06) 1.500849 1.509759 1.506717
##    median       uq      max neval cld
##  1.000000 1.000000 1.000000   100  a 
##  1.522478 1.523124 1.553303   100   b</code></pre>
</div>
<div id="q1b" class="section level3">
<h3>Q1b</h3>
<p>Since choosing <span class="math inline">\(\lambda\)</span> requires a more formal way to do so, we implement a cross validation function to use.</p>
<pre class="r"><code>cv_error &lt;- function(y, x, lambdas, k = 10) {
  # define error metric
  mse &lt;- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors &lt;- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds &lt;- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error &lt;- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test &lt;- x[which(folds == fold)]
      y_test &lt;- y[which(folds == fold)]
      x_train &lt;- x[which(folds != fold)]
      y_train &lt;- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit &lt;- meanKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error &lt;- error + (mse(fit, y_test) / k)
    }
    errors[i] &lt;- error
  }
  return(errors)
}</code></pre>
<p>Find the lambda which has the smallest cross-validation error.</p>
<pre class="r"><code>lambdas &lt;- seq(0.001, 0.025, length.out = 50)
errors_krs &lt;- cv_error(y, x, lambdas, 10)
# lambda_min_krs &lt;- lambdas[which.min(cv)]</code></pre>
<p>Plot the cross validation error curve and the lambda that gives the smallest error.</p>
<pre class="r"><code>data.frame(lambda = lambdas, error = errors_krs) %&gt;%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21,size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = &quot;darkgrey&quot;,linetype=&#39;dashed&#39;)</code></pre>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><code>C</code> implementation of cross validation.</p>
<pre class="rcpp"><code>// mean squared error
double mse(const NumericVector a, const NumericVector b){
  return mean(pow((a-b),2));
}

// random wrapper
int randWrapper(const int n){
  return(floor(unif_rand()));
}

// create a sequence of n samples from 0 to k
// [[Rcpp::export(name = &quot;sample_rcpp&quot;)]]
NumericVector sample_rcpp(int k, int n){
  NumericVector seq(n);

  // create ordered sequence
  for(int i = 1; i &lt;= k; i++){
    for(int j = i; j &lt;= n; j= j + k){
      seq[j-1] = i-1;
    }
  }

  // randomly shuffle the sequence of indeces
  std::random_shuffle(seq.begin(), seq.end(), randWrapper);
  return seq;
}

// cross validation function

// [[Rcpp::export(name = &quot;cv_meanKRS_cpp&quot;)]]
NumericVector cv_meanKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector lambdas, const int k){
  int n = x.size();
  int m = lambdas.size();

  NumericVector errors(m);
  NumericVector folds = sample_rcpp(k, n);

  // iterate over lambdas
  for(int i=0; i &lt; m; i++){
    double error = 0;

    // iterate over folds
    for(int j = 0; j &lt; k; j++){

      // split the data into train and test set
      NumericVector x_test = x[(folds==j)];
      NumericVector x_train = x[(folds!=j)];
      NumericVector y_test = y[(folds==j)];
      NumericVector y_train = y[(folds!=j)];

      // fit model on training data and get
      NumericVector fit = meanKRSR_cpp(y_train, x_train, x_test, lambdas[i]);

      // calculate error for fold
      error = error + mse(fit, y_test)/k;
    }
    errors[i] = error;
  }

  return errors;
}</code></pre>
<p>Source the <code>C</code> file.</p>
<pre class="r"><code>sourceCpp(&quot;../code/cv_meanKRS.cpp&quot;)</code></pre>
<p>Use the <code>C</code> implementation to perform the cross validation.</p>
<pre class="r"><code>errors_krs_cpp &lt;- cv_meanKRS_cpp(y, x, lambdas, 10)</code></pre>
<p>Plot the cross validation error curve.</p>
<pre class="r"><code>data.frame(lambda = lambdas, error = errors_krs_cpp) %&gt;%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = &quot;darkgrey&quot;,linetype=&#39;dashed&#39;)</code></pre>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Compare the efficiency of the <code>R</code> and <code>C</code> implementations.</p>
<pre class="r"><code>microbenchmark(cv_meanKRS_cpp(y, x, lambdas, 5), cv_error(y, x, lambdas, 5), unit = &quot;relative&quot;)</code></pre>
<p>###Q2</p>
<p>Implement the KRS with a variable <span class="math inline">\(\lambda\)</span> value.</p>
<pre class="r"><code>varKRS &lt;- function(y, x, x0, lam) {
  n &lt;- length(x)
  n0 &lt;- length(x0)
  mu &lt;- numeric(n)

  out &lt;- numeric(n0)
  madHat &lt;- numeric(n0)

  for (i in 1:n) {
    kernel &lt;- dnorm(x, x[i], lam)
    mu[i] &lt;- sum(kernel * y) / sum(kernel)
  }

  resAbs &lt;- abs(y - mu)
  for (ii in 1:n0) {
    kernel &lt;- dnorm(x, x0[ii], lam)
    madHat[ii] &lt;- sum(kernel * resAbs) / sum(kernel)
  }

  w &lt;- 1 / madHat
  w &lt;- w / mean(w)

  for (i in 1:n0) {
    kernel &lt;- dnorm(x, x0[i], lam * w[i])
    out[i] &lt;- sum(kernel * y) /
      sum(kernel)
  }

  return(out)
}</code></pre>
<p>Fit the adaptive smooth using the <code>R</code> implementation.</p>
<pre class="r"><code>muSmoothAdapt &lt;- varKRS(y = y, x = x, x0 = x_new, lam = 0.06)</code></pre>
<p>Plot the fit of all smooths.</p>
<pre class="r"><code>df_new &lt;- data.frame(x_new, muSmoothLarge, muSmoothSmall, muSmoothAdapt)

ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothLarge, colour = &quot;large&quot;), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothSmall, colour = &quot;small&quot;), size = 0.8) +
  geom_line(data = df_new, aes(x = x_new, y = muSmoothAdapt, colour = &quot;adapt&quot;), size = 0.8) +
  scale_color_manual(name = &quot;Lambda&quot;, values = c(large = &quot;#ffd400&quot;, small = &quot;#19c6c1&quot;, adapt = &quot;#ef3939&quot;)) +
  labs(title = &quot;Fitted smooths using R implementation&quot;)</code></pre>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Writing the function in <code>C</code></p>
<pre class="rcpp"><code>#include &lt;Rcpp.h&gt;
#include &lt;Rmath.h&gt;
using namespace Rcpp;

// [[Rcpp::export(name = &quot;varKRS_cpp&quot;)]]
NumericVector varKRS_cpp(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda){
  int n = x.size();
  int n0 = x0.size();
  NumericVector res(n);
  NumericVector mu(n);

  NumericVector madHat(n0);
  NumericVector out(n0);

  for(int i = 0; i &lt; n; i++){
    NumericVector kernel = dnorm(x, x[i],lambda,0);

    double num = 0;
    for(int j = 0; j &lt; n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j &lt; n; j++){
      denom = denom + kernel[j];
    }
    mu[i] = num / denom;
  }

  NumericVector resAbs(n);
  resAbs = abs(y - mu);


  NumericVector w(n0);
  double sum_w = 0;

  for(int i=0; i &lt; n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda,0);

    double num = 0;
    for(int j = 0; j &lt; n; j++){
      num = num + kernel[j] * resAbs[j];
    }

    double denom = 0;
    for(int j = 0; j &lt; n; j++){
      denom = denom + kernel[j];
    }
    madHat[i] = num / denom;
    w[i] = denom / num;
    sum_w = sum_w + w[i];

  }

  for(int i = 0; i &lt; n0; i++){
    w[i] = n0 * w[i] / sum_w;
  }

  for(int i=0; i &lt; n0; i++){
    NumericVector kernel = dnorm(x, x0[i],lambda*w[i],0);

    double num = 0;
    for(int j = 0; j &lt; n; j++){
      num = num + kernel[j] * y[j];
    }

    double denom = 0;
    for(int j = 0; j &lt; n; j++){
      denom = denom + kernel[j];
    }
    out[i] = num / denom;
  }


  return out;
}</code></pre>
<p>Source the <code>C</code> file for the above function.</p>
<pre class="r"><code>sourceCpp(&quot;../code/varKRS.cpp&quot;)</code></pre>
<p>Fit the model.</p>
<pre class="r"><code>muSmoothAdapt_rcpp &lt;- varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06)</code></pre>
<p>Check that it returns the same fitted values as the <code>R</code> implementation.</p>
<pre class="r"><code>max(abs(muSmoothAdapt - muSmoothAdapt_rcpp))</code></pre>
<pre><code>## [1] 2.109424e-15</code></pre>
<p>Comparing the performances it is clear that the <code>C</code> implementation is faster.</p>
<pre class="r"><code>microbenchmark(varKRS(y = y, x = x, x0 = x_new, lam = 0.06), varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06), unit = &quot;relative&quot;)</code></pre>
<pre><code>## Unit: relative
##                                              expr      min       lq    mean
##      varKRS(y = y, x = x, x0 = x_new, lam = 0.06) 1.522923 1.574803 1.59361
##  varKRS_cpp(y = y, x = x, x0 = x_new, lam = 0.06) 1.000000 1.000000 1.00000
##    median       uq      max neval cld
##  1.534016 1.597117 3.914591   100   b
##  1.000000 1.000000 1.000000   100  a</code></pre>
<p>Perform cross validation using <code>R</code></p>
<pre class="r"><code>cv_error_var &lt;- function(y, x, lambdas, k = 10) {
  # define error metric
  mse &lt;- function(a, b) {
    mean((a - b)^2)
  }

  # vector to store errors for every lambda
  errors &lt;- rep(0, length(lambdas))

  # create a set of random indeces for the folds
  folds &lt;- sample(1:k, size = length(x), replace = TRUE)

  # calculate the error for every lambda
  for (i in 1:length(lambdas)) {
    error &lt;- 0

    # calculate error for every fold
    for (fold in 1:k) {
      # split data into training and test data
      x_test &lt;- x[which(folds == fold)]
      y_test &lt;- y[which(folds == fold)]
      x_train &lt;- x[which(folds != fold)]
      y_train &lt;- y[which(folds != fold)]

      # fit model on the training set and get predictions for the training set
      fit &lt;- varKRS(y = y_train, x = x_train, x0 = x_test, lam = lambdas[i])

      # calculate the average mse
      error &lt;- error + (mse(fit, y_test) / k)
    }
    errors[i] &lt;- error
  }
  return(errors)
}</code></pre>
<p>Compute the errors</p>
<pre class="r"><code>errors_var &lt;- cv_error_var(y, x, lambdas, 10)</code></pre>
<p>Plot the cross validation error curve.</p>
<pre class="r"><code>data.frame(lambda = lambdas, error = errors_var) %&gt;%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5)</code></pre>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>  geom_vline(aes(xintercept = lambda[which.min(error)]), color = &quot;darkgrey&quot;,linetype=&#39;dashed&#39;)</code></pre>
<pre><code>## mapping: xintercept = ~lambda[which.min(error)] 
## geom_vline: na.rm = FALSE
## stat_identity: na.rm = FALSE
## position_identity</code></pre>
<p>Comparing the performance of the <code>R</code> and the <code>C</code> implementation of the cross validation we get</p>
<pre class="r"><code>sourceCpp(&quot;../code/cv_varKRS.cpp&quot;)</code></pre>
<p>Compute the errors.</p>
<pre class="r"><code>errors_var_cpp &lt;- cv_varKRS_cpp(y, x, lambdas, 10)</code></pre>
<p>Plot the cross validation error curve.</p>
<pre class="r"><code>data.frame(lambda = lambdas, error = errors_var_cpp) %&gt;%
  ggplot(aes(x = lambda, y = error)) +
  geom_point(shape=21, size=1.5) +
  geom_vline(aes(xintercept = lambda[which.min(error)]), color = &quot;darkgrey&quot;, linetype=&#39;dashed&#39;)</code></pre>
<p><img src="/post/2020-05-25-rcpp_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>microbenchmark(cv_varKRS_cpp(y, x, lambdas, 5), cv_error_var(y, x, lambdas, 5), unit = &quot;relative&quot;)</code></pre>
</div>

    
</div>

    
<div class="footer">


    
        <div class="tags">
            <i class="fa fa-tags"></i>
            <div class="links">
                
                    
                    
                    <a href="/tags/rcpp/">rcpp</a>
                    
                
            </div>
        </div>
    

    
</div>

</article>

        
    </div>

    
        <div id="comments-container">
            
            

        </div>
    

     

    </div>

    
<footer>
    <div class="container">

        

        

        <div class="right">
            

            
        </div>
    </div>
</footer>


<div class="credits">
    <div class="container">
        <div class="copyright">
            <a href="https://github.com/Lednerb" target="_blank">
                &copy;
                
                2017
                
                by Lednerb
            </a>
            
        </div>
        <div class="author">
            <a href="https://github.com/Lednerb/bilberry-hugo-theme"
                target="_blank">Bilberry Hugo Theme</a>
        </div>
    </div>
</div>


    

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']]}
        });
</script>

    


    <script type="text/javascript" src="/theme.js"></script>

    
    
    

    


</body>

</html>
